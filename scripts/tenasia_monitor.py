#!/usr/bin/env python3
"""
í…ì•„ì‹œì•„ ê¸°ì‚¬ ëª¨ë‹ˆí„°ë§ & ìˆ˜ì§‘ ë„êµ¬
====================================
í…ì•„ì‹œì•„(tenasia.co.kr)ì—ì„œ ê¸°ì‚¬ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³ 
í‚¤ì›Œë“œë³„ë¡œ í•„í„°ë§í•˜ì—¬ CSV/JSONìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
  1. í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜: pip install requests beautifulsoup4 schedule
  2. ì•„ë˜ KEYWORDS ë¦¬ìŠ¤íŠ¸ì— ëª¨ë‹ˆí„°ë§í•  í‚¤ì›Œë“œë¥¼ ì¶”ê°€
  3. ì‹¤í–‰: python tenasia_monitor.py

ì£¼ìš” ê¸°ëŠ¥:
  - ì¹´í…Œê³ ë¦¬ë³„ ìµœì‹  ê¸°ì‚¬ ìˆ˜ì§‘ (ë®¤ì§, ë“œë¼ë§ˆ, ì—°ì˜ˆê°€í™”ì œ ë“±)
  - í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§ (ì•„í‹°ìŠ¤íŠ¸ëª…, ì†Œì†ì‚¬ëª… ë“±)
  - CSV & JSON ìë™ ì €ì¥
  - ì¤‘ë³µ ê¸°ì‚¬ ìë™ ì œê±°
  - ìŠ¤ì¼€ì¤„ë§ (ë§¤ 30ë¶„ë§ˆë‹¤ ìë™ ìˆ˜ì§‘)
  - íŠ¸ë Œë“œ ë¦¬í¬íŠ¸ ìë™ ìƒì„±
"""

import requests
from bs4 import BeautifulSoup
import json
import csv
import os
import re
from datetime import datetime, timedelta
from collections import Counter
from urllib.parse import urljoin

# ============================================================
# âš™ï¸ ì„¤ì • - ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”!
# ============================================================

# ëª¨ë‹ˆí„°ë§í•  í‚¤ì›Œë“œ (ì•„í‹°ìŠ¤íŠ¸ëª…, ì†Œì†ì‚¬ëª…, í”„ë¡œê·¸ë¨ëª… ë“±)
KEYWORDS = [
    "BTS", "ë°©íƒ„ì†Œë…„ë‹¨", "ì•„ì´ë¸Œ", "IVE", "ë‰´ì§„ìŠ¤", "NewJeans",
    "ìŠ¤íŠ¸ë ˆì´í‚¤ì¦ˆ", "ì—ìŠ¤íŒŒ", "aespa", "ì„¸ë¸í‹´", "SEVENTEEN",
    "ë¸”ë™í•‘í¬", "BLACKPINK", "ì•„ì´ë“¤", "(G)I-DLE",
    "í•˜ì´ë¸Œ", "SM", "JYP", "YG", "ì¹´ì¹´ì˜¤",
    # ì›í•˜ëŠ” í‚¤ì›Œë“œë¥¼ ì¶”ê°€í•˜ì„¸ìš”
]

# ìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬ URL
CATEGORIES = {
    "ì—°ì˜ˆê°€í™”ì œ": "https://www.tenasia.co.kr/topic",
    "ë®¤ì§": "https://www.tenasia.co.kr/music",
    "ë“œë¼ë§ˆì˜ˆëŠ¥": "https://www.tenasia.co.kr/tv-drama",
    "ì˜í™”": "https://www.tenasia.co.kr/movie",
    "ì—”í„°ë¹„ì¦ˆ": "https://www.tenasia.co.kr/enterbiz",
}

# ë°ì´í„° ì €ì¥ ê²½ë¡œ
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "data")
CSV_FILE = os.path.join(DATA_DIR, "articles.csv")
JSON_FILE = os.path.join(DATA_DIR, "articles.json")
REPORT_FILE = os.path.join(DATA_DIR, "trend_report.json")

# ìˆ˜ì§‘ ê°„ê²© (ë¶„)
INTERVAL_MINUTES = 30

# ìš”ì²­ í—¤ë”
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
}

# ============================================================
# í•µì‹¬ ê¸°ëŠ¥
# ============================================================

def ensure_data_dir():
    """ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±"""
    os.makedirs(DATA_DIR, exist_ok=True)


def load_existing_articles():
    """ê¸°ì¡´ì— ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡ ë¡œë“œ"""
    if os.path.exists(JSON_FILE):
        with open(JSON_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []


def get_existing_urls(articles):
    """ì´ë¯¸ ìˆ˜ì§‘ëœ URL ì„¸íŠ¸ ë°˜í™˜"""
    return set(a.get("url", "") for a in articles)


def fetch_category_page(category_name, url):
    """ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘"""
    articles = []
    try:
        print(f"  ğŸ“¡ [{category_name}] í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘... {url}")
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ (tenasia.co.kr/article/ íŒ¨í„´)
        for link in soup.find_all("a", href=True):
            href = link.get("href", "")
            full_url = urljoin(url, href)

            if "/article/" not in full_url:
                continue

            # ì œëª© ì¶”ì¶œ
            title = ""
            # ì œëª©ì´ ë  ìˆ˜ ìˆëŠ” ìš”ì†Œë“¤ íƒìƒ‰
            heading = link.find(["h1", "h2", "h3", "h4", "h5", "strong"])
            if heading:
                title = heading.get_text(strip=True)
            elif link.get_text(strip=True):
                title = link.get_text(strip=True)

            if not title or len(title) < 5:
                continue

            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            img = link.find("img")
            thumbnail = img.get("src", "") if img else ""

            articles.append({
                "title": title,
                "url": full_url,
                "category": category_name,
                "thumbnail": thumbnail,
            })

        print(f"  âœ… [{category_name}] {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")

    except Exception as e:
        print(f"  âŒ [{category_name}] ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    return articles


def fetch_article_detail(url):
    """ê°œë³„ ê¸°ì‚¬ ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘"""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì¼ë°˜ì ì¸ ê¸°ì‚¬ ë³¸ë¬¸ ì„ íƒì)
        content = ""
        # ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        for selector in ["article", ".article-body", ".article-content",
                         "#article-body", ".news-body", ".entry-content",
                         ".post-content", "[itemprop='articleBody']"]:
            body = soup.select_one(selector)
            if body:
                content = body.get_text(separator="\n", strip=True)
                break

        if not content:
            # fallback: <p> íƒœê·¸ ëª¨ì•„ì„œ ì¶”ì¶œ
            paragraphs = soup.find_all("p")
            content = "\n".join(p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 20)

        # ê¸°ì ì´ë¦„ ì¶”ì¶œ
        journalist = ""
        journalist_match = re.search(r"(\S+)\s*í…ì•„ì‹œì•„\s*ê¸°ì", content)
        if journalist_match:
            journalist = journalist_match.group(1)

        # ë‚ ì§œ ì¶”ì¶œ
        date_str = ""
        time_tag = soup.find("time")
        if time_tag:
            date_str = time_tag.get("datetime", time_tag.get_text(strip=True))
        if not date_str:
            # meta íƒœê·¸ì—ì„œ ì‹œë„
            meta_date = soup.find("meta", {"property": "article:published_time"})
            if meta_date:
                date_str = meta_date.get("content", "")

        return {
            "content_preview": content[:500] if content else "",
            "journalist": journalist,
            "published_date": date_str,
        }

    except Exception as e:
        print(f"    âš ï¸ ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return {"content_preview": "", "journalist": "", "published_date": ""}


def match_keywords(title, content_preview, keywords):
    """í‚¤ì›Œë“œ ë§¤ì¹­ - ì œëª©ì´ë‚˜ ë³¸ë¬¸ì— í¬í•¨ëœ í‚¤ì›Œë“œ ë°˜í™˜"""
    text = (title + " " + content_preview).upper()
    matched = []
    for kw in keywords:
        if kw.upper() in text:
            matched.append(kw)
    return matched


def collect_all_articles(fetch_details=True):
    """ì „ì²´ ì¹´í…Œê³ ë¦¬ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
    print(f"\n{'='*60}")
    print(f"ğŸ” í…ì•„ì‹œì•„ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")

    existing_articles = load_existing_articles()
    existing_urls = get_existing_urls(existing_articles)
    new_articles = []

    for cat_name, cat_url in CATEGORIES.items():
        raw_articles = fetch_category_page(cat_name, cat_url)

        for article in raw_articles:
            if article["url"] in existing_urls:
                continue

            # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (ì„ íƒì )
            if fetch_details:
                print(f"    ğŸ“– ìƒì„¸ ìˆ˜ì§‘: {article['title'][:40]}...")
                details = fetch_article_detail(article["url"])
                article.update(details)

            # í‚¤ì›Œë“œ ë§¤ì¹­
            matched = match_keywords(
                article.get("title", ""),
                article.get("content_preview", ""),
                KEYWORDS
            )
            article["matched_keywords"] = matched
            article["collected_at"] = datetime.now().isoformat()

            new_articles.append(article)
            existing_urls.add(article["url"])

    # ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸°
    all_articles = existing_articles + new_articles

    print(f"\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: ì‹ ê·œ {len(new_articles)}ê°œ / ì „ì²´ {len(all_articles)}ê°œ")

    return all_articles, new_articles


def save_to_json(articles):
    """JSON íŒŒì¼ë¡œ ì €ì¥"""
    with open(JSON_FILE, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ JSON ì €ì¥: {JSON_FILE}")


def save_to_csv(articles):
    """CSV íŒŒì¼ë¡œ ì €ì¥"""
    if not articles:
        return

    fieldnames = [
        "title", "url", "category", "matched_keywords",
        "journalist", "published_date", "collected_at",
        "content_preview", "thumbnail"
    ]

    with open(CSV_FILE, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
        writer.writeheader()
        for article in articles:
            row = article.copy()
            # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            if isinstance(row.get("matched_keywords"), list):
                row["matched_keywords"] = ", ".join(row["matched_keywords"])
            writer.writerow(row)
    print(f"ğŸ’¾ CSV ì €ì¥: {CSV_FILE}")


def generate_trend_report(articles):
    """íŠ¸ë Œë“œ ë¦¬í¬íŠ¸ ìƒì„±"""
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ íŠ¸ë Œë“œ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    print(f"{'='*60}")

    now = datetime.now()
    week_ago = now - timedelta(days=7)

    # ì´ë²ˆ ì£¼ ê¸°ì‚¬ë§Œ í•„í„°
    recent = []
    for a in articles:
        collected = a.get("collected_at", "")
        if collected:
            try:
                dt = datetime.fromisoformat(collected)
                if dt >= week_ago:
                    recent.append(a)
            except:
                recent.append(a)  # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ í¬í•¨
        else:
            recent.append(a)

    # 1. í‚¤ì›Œë“œë³„ ì–¸ê¸‰ íšŸìˆ˜
    keyword_counts = Counter()
    for a in recent:
        for kw in a.get("matched_keywords", []):
            keyword_counts[kw] += 1

    # 2. ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜
    category_counts = Counter()
    for a in recent:
        category_counts[a.get("category", "ê¸°íƒ€")] += 1

    # 3. ê¸°ìë³„ ê¸°ì‚¬ ìˆ˜
    journalist_counts = Counter()
    for a in recent:
        j = a.get("journalist", "")
        if j:
            journalist_counts[j] += 1

    # 4. ì¼ë³„ ê¸°ì‚¬ ìˆ˜
    daily_counts = Counter()
    for a in recent:
        collected = a.get("collected_at", "")
        if collected:
            try:
                day = datetime.fromisoformat(collected).strftime("%Y-%m-%d")
                daily_counts[day] += 1
            except:
                pass

    # ë¦¬í¬íŠ¸ ë°ì´í„°
    report = {
        "generated_at": now.isoformat(),
        "period": f"{week_ago.strftime('%Y-%m-%d')} ~ {now.strftime('%Y-%m-%d')}",
        "total_articles": len(recent),
        "top_keywords": keyword_counts.most_common(20),
        "category_breakdown": dict(category_counts),
        "top_journalists": journalist_counts.most_common(10),
        "daily_article_count": dict(sorted(daily_counts.items())),
        "keyword_articles": {},  # í‚¤ì›Œë“œë³„ ëŒ€í‘œ ê¸°ì‚¬
    }

    # í‚¤ì›Œë“œë³„ ëŒ€í‘œ ê¸°ì‚¬ (ìƒìœ„ 5ê°œ í‚¤ì›Œë“œ)
    for kw, count in keyword_counts.most_common(5):
        kw_articles = [
            {"title": a["title"], "url": a["url"], "category": a.get("category", "")}
            for a in recent
            if kw in a.get("matched_keywords", [])
        ][:5]
        report["keyword_articles"][kw] = kw_articles

    # ì €ì¥
    with open(REPORT_FILE, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ íŠ¸ë Œë“œ ë¦¬í¬íŠ¸ ì €ì¥: {REPORT_FILE}")

    # ì½˜ì†” ì¶œë ¥
    print(f"\nğŸ“Š ì£¼ê°„ íŠ¸ë Œë“œ ë¦¬í¬íŠ¸ ({report['period']})")
    print(f"   ì´ ìˆ˜ì§‘ ê¸°ì‚¬: {report['total_articles']}ê±´")

    print(f"\nğŸ”¥ TOP í‚¤ì›Œë“œ:")
    for kw, count in keyword_counts.most_common(10):
        bar = "â–ˆ" * count
        print(f"   {kw:15s} | {bar} ({count}ê±´)")

    print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„:")
    for cat, count in category_counts.most_common():
        print(f"   {cat:10s} | {count}ê±´")

    print(f"\nâœï¸ í™œë°œí•œ ê¸°ì:")
    for j, count in journalist_counts.most_common(5):
        print(f"   {j} ({count}ê±´)")

    return report


def run_once(fetch_details=True):
    """í•œë²ˆ ì‹¤í–‰"""
    ensure_data_dir()
    all_articles, new_articles = collect_all_articles(fetch_details=fetch_details)
    save_to_json(all_articles)
    save_to_csv(all_articles)
    report = generate_trend_report(all_articles)

    # í‚¤ì›Œë“œ ë§¤ì¹­ëœ ì‹ ê·œ ê¸°ì‚¬ ì•Œë¦¼
    keyword_matched = [a for a in new_articles if a.get("matched_keywords")]
    if keyword_matched:
        print(f"\nğŸ”” í‚¤ì›Œë“œ ë§¤ì¹­ ì‹ ê·œ ê¸°ì‚¬ {len(keyword_matched)}ê±´:")
        for a in keyword_matched:
            kws = ", ".join(a["matched_keywords"])
            print(f"   [{kws}] {a['title']}")
            print(f"   â†’ {a['url']}")
    else:
        print(f"\nğŸ’¤ í‚¤ì›Œë“œ ë§¤ì¹­ ì‹ ê·œ ê¸°ì‚¬ ì—†ìŒ")

    return report


def run_scheduled():
    """ìŠ¤ì¼€ì¤„ë§ ëª¨ë“œë¡œ ì‹¤í–‰"""
    try:
        import schedule
        import time

        print(f"â° ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘: {INTERVAL_MINUTES}ë¶„ ê°„ê²©ìœ¼ë¡œ ìˆ˜ì§‘")
        print(f"   ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”\n")

        # ì¦‰ì‹œ 1íšŒ ì‹¤í–‰
        run_once()

        # ìŠ¤ì¼€ì¤„ ë“±ë¡
        schedule.every(INTERVAL_MINUTES).minutes.do(run_once)

        while True:
            schedule.run_pending()
            time.sleep(1)

    except ImportError:
        print("âš ï¸ schedule íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. pip install schedule ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        print("   ë‹¨ì¼ ì‹¤í–‰ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.\n")
        run_once()
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ")


# ============================================================
# ì‹¤í–‰
# ============================================================

if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--schedule":
        run_scheduled()
    elif len(sys.argv) > 1 and sys.argv[1] == "--quick":
        # ìƒì„¸ ìˆ˜ì§‘ ì—†ì´ ë¹ ë¥´ê²Œ (ì œëª©+URLë§Œ)
        run_once(fetch_details=False)
    else:
        # ê¸°ë³¸: ìƒì„¸ ìˆ˜ì§‘ í¬í•¨ 1íšŒ ì‹¤í–‰
        run_once(fetch_details=True)
